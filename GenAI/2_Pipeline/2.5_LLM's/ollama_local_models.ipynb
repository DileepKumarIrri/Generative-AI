{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "To download any models and use them from Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-ollama in c:\\users\\admin\\anaconda3\\envs\\venv\\lib\\site-packages (0.3.0)\n",
      "Requirement already satisfied: ollama<1,>=0.4.4 in c:\\users\\admin\\anaconda3\\envs\\venv\\lib\\site-packages (from langchain-ollama) (0.4.7)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.47 in c:\\users\\admin\\anaconda3\\envs\\venv\\lib\\site-packages (from langchain-ollama) (0.3.49)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\admin\\anaconda3\\envs\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.47->langchain-ollama) (0.2.11)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\admin\\anaconda3\\envs\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.47->langchain-ollama) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\admin\\anaconda3\\envs\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.47->langchain-ollama) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\admin\\anaconda3\\envs\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.47->langchain-ollama) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from langchain-core<1.0.0,>=0.3.47->langchain-ollama) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\admin\\anaconda3\\envs\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.47->langchain-ollama) (4.12.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\admin\\anaconda3\\envs\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.47->langchain-ollama) (2.10.6)\n",
      "Requirement already satisfied: httpx<0.29,>=0.27 in c:\\users\\admin\\anaconda3\\envs\\venv\\lib\\site-packages (from ollama<1,>=0.4.4->langchain-ollama) (0.27.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\admin\\anaconda3\\envs\\venv\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (4.8.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\admin\\anaconda3\\envs\\venv\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\admin\\anaconda3\\envs\\venv\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\admin\\anaconda3\\envs\\venv\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (3.10)\n",
      "Requirement already satisfied: sniffio in c:\\users\\admin\\anaconda3\\envs\\venv\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\admin\\anaconda3\\envs\\venv\\lib\\site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\admin\\anaconda3\\envs\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.47->langchain-ollama) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\admin\\anaconda3\\envs\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-ollama) (3.10.14)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\admin\\anaconda3\\envs\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-ollama) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\admin\\anaconda3\\envs\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-ollama) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\admin\\anaconda3\\envs\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.47->langchain-ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\admin\\anaconda3\\envs\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.47->langchain-ollama) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\envs\\venv\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-ollama) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\envs\\venv\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-ollama) (2.3.0)\n"
     ]
    }
   ],
   "source": [
    "# cmd:\n",
    "# >>ollama run deepseek-r1:1.5b\n",
    "! ollama pull deepseek-r1:1.5b\n",
    "! pip install -U langchain-ollama\n",
    "\n",
    "# https://python.langchain.com/api_reference/ollama/chat_models/langchain_ollama.chat_models.ChatOllama.html#langchain_ollama.chat_models.ChatOllama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling aabd4debf0c8... 100% ▕████████████████▏ 1.1 GB                         \n",
      "pulling 369ca498f347... 100% ▕████████████████▏  387 B                         \n",
      "pulling 6e4c38e1172f... 100% ▕████████████████▏ 1.1 KB                         \n",
      "pulling f4d24e9138dd... 100% ▕████████████████▏  148 B                         \n",
      "pulling a85fe2a2e58e... 100% ▕████████████████▏  487 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "browser-use 0.1.28 requires langchain==0.3.14, but you have langchain 0.3.21 which is incompatible.\n",
      "browser-use 0.1.28 requires langchain-ollama==0.2.2, but you have langchain-ollama 0.3.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "! ollama pull deepseek-r1:1.5b\n",
    "! pip install -qU langchain_ollama "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model = \"deepseek-r1:1.5b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usage**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Sync invoke**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Alright, let's see how I can translate \"I love programming\" into Telugu. First, I need to understand what each part means. \"I love programming\" is asking someone about their passion for coding or programming.\n",
      "\n",
      "In English, words like \"love,\" \"programming,\" and \"coding\" are pretty straightforward. But translating them accurately is key. Let me break it down:\n",
      "\n",
      "1. \"I\": In Telugu, this is simply \"మొత్పు.\" It's the indefinite article meaning anyone.\n",
      "\n",
      "2. \"love\": The idiomatic phrase in English for love translates to \"దనలే\" in Telugu. It conveys feelings of happiness or affection.\n",
      "\n",
      "3. \"programming\": This term has multiple meanings, but in the context of coding, it refers to software development. So I should use a precise translation that implies an interest in programming.\n",
      "\n",
      "Putting it all together: \"మొత్పు దనలే నిష్దేనం కలగిస్తుంటా\" means \"I love the idea of programming and it's time to code.\" It captures the passion for coding and expressing that desire in Telugu.\n",
      "\n",
      "I should make sure the translation sounds natural and maintains the intent. Let me read it again: \"మొత్పు దనలే నిష్దేనం కలగిస్తుంటా.\" Yes, that seems to fit perfectly.\n",
      "</think>\n",
      "\n",
      "మొత్పు దనలే నిష్దేనం కలగిస్తుంటా\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    (\"system\", \"You are a helpful translator. Translate the user sentence to Telugu.\"),\n",
    "    (\"human\", \"I love programming\"),\n",
    "]\n",
    "result=llm.invoke(messages)  # Prompt: Translate the sentence into Hindi. Human: I love programming.\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Sync Stream**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to translate \"How are you?\" into Telugu. Hmm, where do I start? Well, I know that in English, when someone says \"how are you,\" they're asking for their current state or satisfaction with something.\n",
      "\n",
      "I remember learning that in the Bible, they say \"K Vishnu\" for wisdom and \"K Shikha\" for judgment. So maybe I should use those terms in translation to add a more spiritual or traditional feel to it?\n",
      "\n",
      "Wait, but does that even translate correctly? Let me think about how each word is translated.\n",
      "\n",
      "The first part, \"How are you,\" translates to \"Kalikasammi.\" That sounds right because \"how\" is often translated as \"kalikas,\" and \"are\" might be \"sammi.\"\n",
      "\n",
      "Now the second part, \"you.\" In Telugu, we use different words depending on the context. Since it's a question asking about someone, maybe \"kundama\" would work? I think that means \"you\" in this context.\n",
      "\n",
      "Putting them together, \"How are you?\" becomes \"Kalikasammi Kundama.\" That seems to make sense and fits the traditional way of asking questions in Telugu. \n",
      "\n",
      "I wonder if there's a more modern or casual translation as well, just to check. Maybe something like \"He is not happy.\" But that uses the word \"he,\" which might be considered a bit modern or less formal compared to \"Kalikasammi.\"\n",
      "\n",
      "So overall, I think using \"Kalikasammi\" for \"how are you\" and \"Kundama\" for \"you\" is appropriate. It maintains the original question's meaning while fitting into the Telugu dialect.\n",
      "</think>\n",
      "\n",
      "How are you?"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    (\"system\", \"You are a helpful translator. Translate the user sentence to Telugu.\"),\n",
    "    (\"human\", \"HOw are you?\"),\n",
    "]\n",
    "\n",
    "for chunk in llm.stream(messages):\n",
    "    print(chunk.content, end='', flush=True) # Prompt: Translate the sentence into Hindi. Human: I love programming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Async**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "# Define multiple messages\n",
    "messages_list = [\n",
    "    [(\"system\", \"Translate the user sentence to Hindi.\"), (\"human\", \"How are you?\")],\n",
    "    [(\"system\", \"Translate the user sentence to French.\"), (\"human\", \"What is your name?\")],\n",
    "    [(\"system\", \"Translate the user sentence to Spanish.\"), (\"human\", \"Where do you live?\")],\n",
    "]\n",
    "\n",
    "# Simple async function to process the messages\n",
    "async def process_requests(messages_list):\n",
    "    for messages in messages_list:\n",
    "        response = await llm.ainvoke(messages)\n",
    "        print(response.content)\n",
    "\n",
    "# Run the async function\n",
    "await process_requests(messages_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Async stream**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define multiple messages\n",
    "messages_list = [\n",
    "    [(\"system\", \"Translate the user sentence to Hindi.\"), (\"human\", \"How are you?\")],\n",
    "    [(\"system\", \"Translate the user sentence to French.\"), (\"human\", \"What is your name?\")],\n",
    "    [(\"system\", \"Translate the user sentence to Spanish.\"), (\"human\", \"Where do you live?\")],\n",
    "]\n",
    "\n",
    "# Async function to stream messages\n",
    "async def stream_requests(messages_list):\n",
    "    for messages in messages_list:\n",
    "        async for chunk in llm.astream(messages):\n",
    "            print(chunk.content,end='',flush=True)\n",
    "\n",
    "# Run the async streaming function\n",
    "await stream_requests(messages_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.Async batch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of messages\n",
    "messages_list = [\n",
    "    [(\"system\", \"Translate the user sentence to Hindi.\"), (\"human\", \"How are you?\")],\n",
    "    [(\"system\", \"Translate the user sentence to French.\"), (\"human\", \"What is your name?\")],\n",
    "    [(\"system\", \"Translate the user sentence to Spanish.\"), (\"human\", \"Where do you live?\")],\n",
    "]\n",
    "\n",
    "# Send multiple messages asynchronously in a batch\n",
    "response = await llm.abatch(messages_list)\n",
    "\n",
    "# Loop through the responses and print each content\n",
    "for res in response:\n",
    "    print(res.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 74701a8c35f6... 100% ▕████████████████▏ 1.3 GB                         \n",
      "pulling 966de95ca8a6... 100% ▕████████████████▏ 1.4 KB                         \n",
      "pulling fcc5a6bec9da... 100% ▕████████████████▏ 7.7 KB                         \n",
      "pulling a70ff7e570d9... 100% ▕████████████████▏ 6.0 KB                         \n",
      "pulling 4f659a1e86d7... 100% ▕████████████████▏  485 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "! ollama pull llama3.2:1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(model='llama3.2:1b', modified_at=datetime.datetime(2025, 1, 27, 21, 19, 16, 493866, tzinfo=TzInfo(+05:30)), digest='baf6a787fdffd633537aa2eb51cfd54cb93ff08e28040095462bb63daf552878', size=1321098329, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='1.2B', quantization_level='Q8_0')),\n",
       " Model(model='nomic-embed-text:latest', modified_at=datetime.datetime(2025, 1, 26, 22, 53, 34, 471628, tzinfo=TzInfo(+05:30)), digest='0a109f422b47e3a30ba2b10eca18548e944e8a23073ee3f3e947efcf3c45e59f', size=274302450, details=ModelDetails(parent_model='', format='gguf', family='nomic-bert', families=['nomic-bert'], parameter_size='137M', quantization_level='F16')),\n",
       " Model(model='deepseek-r1:1.5b', modified_at=datetime.datetime(2025, 1, 25, 16, 40, 52, 746978, tzinfo=TzInfo(+05:30)), digest='a42b25d8c10a841bd24724309898ae851466696a7d7f3a0a408b895538ccbc96', size=1117322599, details=ModelDetails(parent_model='', format='gguf', family='qwen2', families=['qwen2'], parameter_size='1.8B', quantization_level='Q4_K_M'))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "print(ollama.list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
