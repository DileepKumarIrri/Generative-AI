{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **HTMLHeaderTextSplitter**:\n",
    "- Splits strictly at headers (`<h1>`, `<h2>`, etc.) and treats them as individual chunks.\n",
    "\n",
    "### 2. **HTMLSectionSplitter**:\n",
    "- Groups related content (header + its subsections) into meaningful sections, maintaining context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Splitting a Local HTML String using HTMLHeaderTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks: 3\n",
      "Maximum chunk size: 106\n",
      "Minimum chunk size: 15\n",
      "\n",
      "Sample Chunk:\n",
      " About This Page\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "\n",
    "file_path = \"C:/Users/admin/OneDrive/Desktop/Chunking_Embedding/Dataset/simple.html\"\n",
    "with open(file_path, encoding='utf-8') as f:\n",
    "    html_string = f.read()\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "html_header_splits = html_splitter.split_text(html_string)\n",
    "chunks = html_header_splits\n",
    "\n",
    "# Get the total chunks, maximum chunk size, minimum chunk size, sample chunk\n",
    "chunk_sizes = [len(chunk.page_content) for chunk in chunks]\n",
    "print(\"Total number of chunks:\", len(chunks))\n",
    "print(\"Maximum chunk size:\", max(chunk_sizes))\n",
    "print(\"Minimum chunk size:\", min(chunk_sizes))\n",
    "\n",
    "print(\"\\nSample Chunk:\\n\", chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Splitting HTML from a URL using HTMLHeaderTextSplitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks: 339 \n",
      "\n",
      "First chunk: page_content='Stanford Encyclopedia of Philosophy  \n",
      "Menu  \n",
      "Browse About Support SEP  \n",
      "Table of Contents What's New Random Entry Chronological Archives  \n",
      "Editorial Information About the SEP Editorial Board How to Cite the SEP Special Characters Advanced Tools Contact  \n",
      "Support the SEP PDFs for SEP Friends Make a Donation SEPIA for Libraries  \n",
      "Entry Navigation  \n",
      "Entry Contents Bibliography Academic Tools Friends PDF Preview Author and Citation Info Back to Top  \n",
      "Kurt GÃ¶del' \n",
      "\n",
      "Maximum chunk size among all: 500 \n",
      "\n",
      "Minimum chunk size among all: 5 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import HTMLHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "# Define headers to split on\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "    (\"h4\", \"Header 4\"),\n",
    "]\n",
    "\n",
    "# Initialize HTMLHeaderTextSplitter and split HTML from URL\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "url = \"https://plato.stanford.edu/entries/goedel/\"\n",
    "html_header_splits = html_splitter.split_text_from_url(url)\n",
    "\n",
    "# Split the chunks further using RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=30)\n",
    "chunks = text_splitter.split_documents(html_header_splits)\n",
    "\n",
    "# Step 6: Print metadata for the first chunk and total number of chunks\n",
    "print(\"Total number of chunks:\", len(chunks),\"\\n\")\n",
    "print(\"First chunk:\", chunks[0],\"\\n\")\n",
    "\n",
    "# Step 7: Print maximum and minimum chunk sizes\n",
    "chunk_sizes = [len(chunk.page_content) for chunk in chunks]\n",
    "print('Maximum chunk size among all:', max(chunk_sizes),\"\\n\")\n",
    "print('Minimum chunk size among all:', min(chunk_sizes),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Splitting HTML content by sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks: 2\n",
      "Maximum chunk size: 148\n",
      "Minimum chunk size: 70\n",
      "\n",
      "Sample Chunk:\n",
      " Welcome to My Simple HTML Page \n",
      " \n",
      " \n",
      " Home \n",
      " About \n",
      " Services \n",
      " Contact\n",
      "---------------------\n",
      "Total number of splits: 2\n",
      "Maximum split size:  148\n",
      "Minimum split size:  70\n",
      "Sample split (1st split): Welcome to My Simple HTML Page \n",
      " \n",
      " \n",
      " Home \n",
      " About \n",
      " Services \n",
      " Contact\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import (RecursiveCharacterTextSplitter, HTMLSectionSplitter)\n",
    "\n",
    "# Step 1: Load the HTML document from file\n",
    "file_path = \"C:/Users/admin/OneDrive/Desktop/Chunking_Embedding/Dataset/simple.html\"\n",
    "with open(file_path, encoding='utf-8') as f:\n",
    "    html_string = f.read()\n",
    "\n",
    "# Step 2: Define headers to split on\n",
    "headers_to_split_on = [(\"h1\", \"Header 1\"), (\"h2\", \"Header 2\")]\n",
    "\n",
    "# Step 3: Use HTMLSectionSplitter to split the document based on headers\n",
    "html_splitter = HTMLSectionSplitter(headers_to_split_on=headers_to_split_on)\n",
    "html_header_splits = html_splitter.split_text(html_string)\n",
    "chunks = html_header_splits\n",
    "\n",
    "# Step 4: Get the total chunks, maximum chunk size, minimum chunk size, and sample chunk\n",
    "chunk_sizes = [len(chunk.page_content) for chunk in chunks]\n",
    "print(\"Total number of chunks:\", len(chunks))\n",
    "print(\"Maximum chunk size:\", max(chunk_sizes))\n",
    "print(\"Minimum chunk size:\", min(chunk_sizes))\n",
    "\n",
    "print(\"\\nSample Chunk:\\n\",chunks[0].page_content)\n",
    "print(\"---------------------\")\n",
    "\n",
    "# Step 5: Use RecursiveCharacterTextSplitter to further split the chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=30)\n",
    "splits = text_splitter.split_documents(html_header_splits)\n",
    "\n",
    "# Step 6: Get the total splits, maximum split size, minimum split size, and sample split\n",
    "split_sizes = [len(split.page_content) for split in splits]\n",
    "print(\"Total number of splits:\",len(splits))\n",
    "print(\"Maximum split size: \",max(split_sizes))\n",
    "print(\"Minimum split size: \",min(split_sizes))\n",
    "# Print a sample split\n",
    "print(f\"Sample split (1st split):\",splits[0].page_content)\n",
    "print(\"---------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
