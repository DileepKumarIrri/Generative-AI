{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks: 13\n",
      "Maximum chunk size: 245\n",
      "Minimum chunk size: 38\n",
      "\n",
      "Sample Chunk:\n",
      " This project aims to develop a real-time monitoring system for cranes in a large-scale foundry. The system integrates sensors, data acquisition units, signal processing units, and software to enhance operational efficiency, predict failures, and\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "# Step 1: Define the headers to split on\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),  # Splits on main headers (H1)\n",
    "    (\"##\", \"Header 2\"), # Splits on subheaders (H2)\n",
    "    (\"###\", \"Header 3\"), # Splits on further subheaders (H3)\n",
    "]\n",
    "\n",
    "# Step 2: Load the long document from file\n",
    "file_path = \"C:/Users/admin/OneDrive/Desktop/Chunking_Embedding/Dataset/project.md\"\n",
    "with open(file_path, encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Step 3: Use MarkdownHeaderTextSplitter to split the document based on headers\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "md_header_splits = markdown_splitter.split_text(text)\n",
    "\n",
    "# Step 4: Use RecursiveCharacterTextSplitter for further chunking based on character length\n",
    "chunk_size = 250\n",
    "chunk_overlap = 30\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "# Split the header-based chunks into smaller chunks\n",
    "chunks = text_splitter.split_documents(md_header_splits)\n",
    "\n",
    "# Get the total chunks, maximum chunk size, minimum chunk size, sample chunk\n",
    "chunk_sizes = [len(chunk.page_content) for chunk in chunks]\n",
    "print(\"Total number of chunks:\", len(chunks))\n",
    "print(\"Maximum chunk size:\", max(chunk_sizes))\n",
    "print(\"Minimum chunk size:\", min(chunk_sizes))\n",
    "\n",
    "print(\"\\nSample Chunk:\\n\",chunks[0].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
